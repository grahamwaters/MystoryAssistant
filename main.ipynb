{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpt2\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "#conda install -c anaconda libomp\n",
    "\n",
    "print(\"Loading GPT-2 model...\")\n",
    "# Load the GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Load the GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Encode the input string as a tensor\n",
    "input_str = \"What is the meaning of life?\"\n",
    "input_tensor = tokenizer.encode(input_str, return_tensors=\"pt\")\n",
    "\n",
    "# Generate a question using the GPT-2 model\n",
    "generated_question = model.generate(\n",
    "    input_tensor,\n",
    "    max_length=100,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True\n",
    ") # this line generates the question using the GPT-2 model\n",
    "\n",
    "# Convert the generated question from a tensor back to a string\n",
    "generated_question_str = tokenizer.decode(generated_question[0], skip_special_tokens=True)\n",
    "print(generated_question_str)\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# if I have a sentence: \"The beaches of France were covered in the bodies of dead soldiers.\"\n",
    "# and I feed it to the model, I want the model to ask questions like:\n",
    "# \"Why were they in France?\"\n",
    "# \"How were they dressed?\"\n",
    "# \"What colors can you see across the sand?\"\n",
    "# \"What was the weather like that day?\"\n",
    "\n",
    "# create a function that takes a sentence and returns a series of questions that can be fed to the model to generate answers\n",
    "def generate_questions(sentence):\n",
    "    # Encode the input string as a tensor\n",
    "    input_tensor = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate a question using the GPT-2 model\n",
    "    # vary the generation seed to get different questions\n",
    "    for i in range(10):\n",
    "        generated_questions = model.generate(\n",
    "            input_tensor,\n",
    "            max_length=100, # maximum length of the generated question\n",
    "            num_beams=200, # number of beams to use (this means that the model will generate 20 different questions, and then choose the best one, based on the model's score)\n",
    "            no_repeat_ngram_size=3, # number of words that can't be repeated in the generated question\n",
    "            early_stopping=False # stop generating questions when the model starts repeating itself\n",
    "        )\n",
    "    generated_questions = model.generate(\n",
    "        input_tensor,\n",
    "        max_length=100, # maximum length of the generated question\n",
    "        num_beams=20, # number of beams to use \n",
    "        no_repeat_ngram_size=3, # number of words that can't be repeated in the generated question\n",
    "        early_stopping=False # stop generating questions when the model starts repeating itself\n",
    "    ) # this line generates the question using the GPT-2 model\n",
    "\n",
    "    # Convert the generated question from a tensor back to a string\n",
    "    generated_questions_str = tokenizer.decode(generated_questions[0], skip_special_tokens=True)\n",
    "    print(generated_questions_str)\n",
    "    return generated_questions_str\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# test the function\n",
    "generate_questions(\"Ask me questions about France.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask me questions about France.\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "generate_questions(\"Ask me questions about France.\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The man was walking down the street.\n",
      "\n",
      "\"I don't know what's going on,\" he said. \"I've never seen anything like this before.\"\n",
      "\n",
      "The man, who asked not to be identified because he was not authorized to speak to the media, said he had no idea what was going on. He said he didn't know who the man was or what he was doing. He also said he did not know how long he had been in the hospital. The man said\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The man was walking down the street.\\n\\n\"I don\\'t know what\\'s going on,\" he said. \"I\\'ve never seen anything like this before.\"\\n\\nThe man, who asked not to be identified because he was not authorized to speak to the media, said he had no idea what was going on. He said he didn\\'t know who the man was or what he was doing. He also said he did not know how long he had been in the hospital. The man said'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_questions(\"The man was walking down the street.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('openai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2bb5586c3dcb926cd49c3292273283dd2b9969ddc5208fcf64781f0159941655"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
